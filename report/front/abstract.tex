\chapter{Abstract}
{\itshape{DeepMind have shown with their AlphaZero algorithm, that it is possible to train a general-purpose deep reinforcement learning agent to gain superhuman playing performance in the games of Chess, Shogi, and Go. I examine if a similar approach, adapted to less processing power, can reach similar results for the game of Hex. Research in general purpose learning agents is of interest because good results in one domain might be translated to good results in others.

During the project several experiments have been run, which in various ways alter the baseline approach slightly. These include: feeding game information to the algorithm in another way; using full pre-activation residual blocks; changing value training target; and mixed precision.

To gain insight into how the various experiments performed, empirical data generated by the experiments are compared to each other, in terms of training loss rates. The game-playing performance of experiments are validated by playing agents against a baseline MCTS agent. The most skilled amongst them have competed in a round-robin tournament with the winner of the 5th Computer Olympiad in 2000 in Hex, Hexy. The empirical data from this tournament has been used to provide Bayesian Elo rating estimations.
}}