\chapter{Related work}
The experiments described in this thesis are primarily related to the AlphaZero algorithm by DeepMind\cite{Silver2018}. All experiments follow a similar algorithmic procedure, but scaled down in terms of games played and training done, to work on consumer grade hardware, in a reasonable time. Likewise, the game complexity of the 7x7 and 9x9 Hex boards are nowhere near that of 19x19 Go. Although the approach is very similar, changes across the different experiments have been made, to investigate the effect of said changes. These are: having a different value training target; a difference residual block design; where game information is fed to the algorithm; and using a mix of single and half precision floating points.

For reinforcement learning agents that also start with tabula rasa knowledge, for the game of Hex, this project is also related to the EXIT algorithm\cite{AnthonyThomasandTianZhengandBarber2017}. There are however some noteworthy differences. The EXIT algorithm is more closely related to AlphaGo Zero, than AlphaZero, in that it has separate networks for value and policy. Interestingly, unlike the DeepMind approaches, and similarly to some of my experiments, EXIT also uses estimates from the neural network as value training target. But it does not add dirichlet noise to policies, and it reinitializes the network weights before it trains on new data. 

The input provided to the network in EXIT includes more game specific features than in this project. In EXIT, the network is given an image of 6 channels: one for the first player's pieces; one for the opponent's; one for first player's pieces connected to one side; one for first player's pieces connected to the opposing side; one for the second player's pieces connected to a third side; and one for the second player's pieces connected to the last side\citep{AnthonyThomasandTianZhengandBarber2017}. The input used in my project is much simpler. Only 2 channels are given to the network, one for the pieces of each player. In this regard, my algorithm gets less domain-specific knowledge compared to EXIT. Finally, EXIT uses a significantly higher amount of MCTS simulations in the tree search, 10,000, as compared to the 200 in this project.