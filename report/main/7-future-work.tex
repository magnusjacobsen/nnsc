\chapter{Future work}
Had there been more time, it could be interesting to let the models train longer, to measure the impact of conducting more games of self-play and perform more training.

It could also be interesting to do experiments with additional parameter tweakings. These could include how the amount of recently played saved games impact the playing performance. \citeauthor{AnthonyThomasandTianZhengandBarber2017} said that their agents, that are somewhat related to the AlphaZero approach, improved from having access to larger datasets\cite{AnthonyThomasandTianZhengandBarber2017}.

Another motivation for future work could be how the learning rate affects ability to play. One could also attempt to divide the learning rate by a constant, when the network experiences training loss plateaus, as done by \citeauthor{He2016}\cite{He2016}. 
When researching this, one should bear in mind that the learning rates for C, D, H, and H, from figure \ref{fig-loss-7} and \ref{fig-loss-9}, experiences plateau-like curve sections, before having a big drop in losses. When implementing a decreasing learning rate it might lead to slower convergence - longer time required to gain the same playing performance.

Doing further tests on what actually happens with prediction accuracy and playing performance, when converting a single precision network to half-precision, is only lightly touched upon in this project. Therefore it would be valuable to measure how much information is being lost, when making the conversion, how it is propagated throughout the algorithm, and how that affects the playing performance of the agent.

Changing the noise levels added during tree search is also a field that would be fascinating to investigate. As well as whether adding noise in other parts of the algorithm helps in learning general patterns for game state evaluation.

In regards to the challenge of only having access to limited processing power, it would be interesting to do additional experiments on how deeper and shallower networks perform. In the same respect, it could also be interesting to see how the trained agents perform, in play mode, when the amount of MCTS simulations are reduced, to increase its decision-making speed.

Finally, it would be interesting to let the experiments E1, F1, G1, and H1, follow suit with H2, and run for extended time, to see how much it improves their playing performances. Since H2 uses single precision both during self-play and while training, it has additional costs in terms of running time, compared to the others.