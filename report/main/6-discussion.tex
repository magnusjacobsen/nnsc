\chapter{Discussion}
The experiments show that the algorithm benefits from being fed more real game information, by using the utility function when reaching a terminal state. Witnessed in reaching a good playing performance earlier. I still see this as being within the general purpose intention behind the algorithm, since \textit{utility} is a generic game function that is also being used for the value training targets in AlphaZero.

It is also clear that having a different value training target than utility of how a game ended is beneficial, at least when it is the tree search mean value that is being used. To do this, however, one has to feed real game information into the algorithm, for instance with the approach mentioned in the above paragraph. One can speculate that having limited discrete end game values does not reflect how a non-terminal game state value is best approximated. The results seem to suggest this.

The full pre-activation residual block design also had an impact on the agents trained on 9x9 boards. The researchers who came up with this design explain that the order of activation functions, whether they occur before or after layers, matters only in networks that have branching in their layers\cite{He2016}. A residual block has branching in the form of the identity mapping of the input added at the end. Their results showed great performance in classifying ImageNet datasets. I argue that it is also shows a positive effect when used on abstract games, when used to learn value and logits, and input is given in the shape of a image of game features. Interestingly, when trained on a less complex board size, 7x7, it did not matter whether the original block design or the full pre-activation was used. And the same goes for the shallow and deep network variants of the full pre-activation experiments, on the 7x7 board. They performed approximately the same. This indicates that doing activations before weight layers has an impact that increases with the difficulty of the prediction task.

Mixed precision also showed some good results. One could assume that a fully single precision agents, H2, or fully half-precision, A1, B1, C1, and D1, would perform better than ones that has to do conversion from single precision master network to a half-precision one, potentially losing accuracy in the process. However, on the 7x7 experiments it did not seem to matter for the networks that used a mean value as value training target. On 9x9 it even had a positive effect for the full pre-activation experiment. This supports the hypotheses from \citeauthor{He2016}, which suggests that full pre-activation improves regularization of network models\cite{He2016}, and \citeauthor{Micikevicius2018} which suggests that converting from single to half-precision may act as a regularization\cite{Micikevicius2018}. However, to say anything conclusive about this more and longer expirements would have had to be run, since H1 and H2 ended up with about the same playing performance. If it is indeed the case, then the other experiments could have been suffering from overfitting to some degree. One could have tested some other values for weight decay and learning rate, to see if they helped in reducing overfitting. But, if full pre-activation and mixed precision does the job, as the results seem to indicate, it is an easier and less domain-specific approach.

It is hard to draw conclusions on the playing performance when going from one game complexity to another, a 7x7 board to 9x9 board, since the baseline MCTS opponent had to be adapted. What can be observed is that the experiment that most closely emulated the original AlphaZero approach, although severely scaled down, had significantly learning issues, compared to the other experiments. Also, full pre-activation seemed to matter more on more complex problems.

In terms of the agents acquiring a game-playing performance that makes them competitive against  state-of-the-art Hex agents, it is clear that 1,200 training iterations have not been sufficient, on the 9x9 board. That said, the findings seem to suggest that doubling the training time, does move the agent in this direction. This is witnessed by H2 with 2,400 training iterations, and 12,000 self-play games played, were able to beat Hexy, set to expert settings. This indicates that 6,000 self-play games is not enough to learn this level of skill, but it is enough, given the right design of the algorithm, to beat the baseline MCTS with 10,000 simulations, quite confidently.

Looking at figure \ref{fig-h2-2400-hexy} in appendix B, the play history indicates that H2, with 2,400 training iterations, has actually learned some strong global virtual connectivity patterns. Virtual connections are plays that create yet to be realized connections, where the opponent is unable to prevent a given outcome. In some cases H2 is stronger than Hexy, but it still has shortcomings, as seen by the game in figure \ref{fig-hexy-h2-2400}.