@misc{Abrams2018,
author = {Abrams, Vish Ishaya},
booktitle = {Medium - Oracle Developers},
title = {{Lessons From AlphaZero (part 4): Improving the Training TargetNo Title}},
url = {https://medium.com/oracledevs/lessons-from-alphazero-part-4-improving-the-training-target-6efba2e71628},
urldate = {2019-05-10},
year = {2018}
}
@article{Anshelevich2000,
abstract = {Abstract The game of Hex is a two-player game with simple rules, a deep underlying mathematical beauty, and a strategic complexity comparable to that of Chess and Go. The massive game -tree search techniques developed mostly for Chess, and successfully used for Checkers, ...},
author = {Anshelevich, Vadim V},
journal = {Scientific American},
title = {{The Game of Hex : An Automatic Theorem Proving Approach to Game Programming}},
year = {2000}
}
@incollection{AnthonyThomasandTianZhengandBarber2017,
author = {Anthony, Thomas and Tian, Zheng and Barber, David},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Garnett, I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R.},
pages = {5360--5370},
publisher = {Curran Associates, Inc.},
title = {{Thinking Fast and Slow with Deep Learning and Tree Search}},
url = {http://papers.nips.cc/paper/7120-thinking-fast-and-slow-with-deep-learning-and-tree-search.pdf},
year = {2017}
}
@misc{Bar2019,
author = {Bar, Chris Scott},
title = {{GeForce RTX 2060 Roundup}},
url = {https://www.nvidia.com/en-us/geforce/news/geforce-rtx-2060-custom-card-roundup/},
urldate = {2019-05-07},
year = {2019}
}
@article{Campbell2002,
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: a single-chip chess search engine, a massively parallel system with multiple levels of parallelism, a strong emphasis on search extensions, a complex evaluation function, and effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue. {\textcopyright} 2001 Elsevier Science B.V. All rights reserved.},
author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng Hsiung},
doi = {10.1016/S0004-3702(01)00129-1},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
title = {{Deep Blue}},
year = {2002}
}
@inproceedings{Chaslot2008,
abstract = {Classic approaches to game AI require either a high quality of domain knowledge, or a long time to generate effective AI behaviour. These two characteristics hamper the goal of establishing challenging game AI. In this paper, we put forward Monte-Carlo Tree Search as a novel, uniﬁed framework to game AI. In the framework, randomized explorations of the search space are used to predict the most promising game actions. We will demonstrate that Monte-Carlo Tree Search can be applied effectively to (1) classic board-games, (2) modern board-games, and (3) video games.},
author = {Chaslot, Guillaume and Bakkes, Sander and Szitai, Istvan and Spronck, Pieter},
booktitle = {Belgian/Netherlands Artificial Intelligence Conference},
issn = {15687805},
title = {{Monte-carlo tree search: A New Framework for Game AI1}},
year = {2008}
}
@article{Committee2008,
abstract = {This standard specifies interchange and arithmetic formats and methods for binary and decimal floating-point arithmetic in computer programming environments. This standard specifies exception conditions and their default handling. An implementation of a floating-point system conforming to this standard may be realized entirely in software, entirely in hardware, or in any combination of software and hardware. For operations specified in the normative part of this standard, numerical results and exceptions are uniquely determined by the values of the input data, sequence of operations, and destination formats, all under user control.},
author = {Committee, Microprocessor Standards},
doi = {10.1109/IEEESTD.2008.4610935},
issn = {01419331},
journal = {IEEE Std 7542008},
title = {{IEEE Standard for Floating-Point Arithmetic}},
year = {2008}
}
@misc{Coulom,
author = {Coulom, R{\'{e}}mi},
title = {{Bayesian Elo Rating}},
url = {https://www.remi-coulom.fr/Bayesian-Elo/},
urldate = {2019-05-10}
}
@inproceedings{Coulom2009,
author = {Coulom, R{\'{e}}mi},
booktitle = {Japanese-French Frontiers of Science Symposium},
title = {{The Monte-Carlo Revolution in Go [presentation]}},
url = {https://www.remi-coulom.fr/JFFoS/JFFoS.pdf},
year = {2009}
}
@misc{GlobalInterpreterLock2017,
author = {GlobalInterpreterLock},
booktitle = {The Python Wiki},
title = {{GlobalInterpreterLock}},
url = {https://wiki.python.org/moin/GlobalInterpreterLock},
urldate = {2019-05-11},
year = {2017}
}
@misc{Hale2019,
author = {Hale, Jeff},
booktitle = {Towards Data Science},
title = {{Which Deep Learning Framework is Growing Fastest?}},
url = {https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318},
urldate = {2019-05-13},
year = {2019}
}
@inproceedings{HeKaimingandZhangXiangyuandRenShaoqingandSun2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1512.03385},
pages = {770--778},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@inproceedings{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_38},
isbn = {9783319464923},
issn = {16113349},
title = {{Identity mappings in deep residual networks}},
year = {2016}
}
@inproceedings{Huang2014,
abstract = {In recent years the Monte Carlo tree search revolution has spread from computer Go to many areas, including computer Hex. MCTS-based Hex players now outperform traditional knowledge-based alpha-beta search players, and the reigning Computer Olympiad Hex gold medallist is the MCTS player MoHex. In this paper we show how to strengthen MoHex, and observe that - as in computer Go - using learned patterns in priors and replacing a hand-crafted simulation policy by a softmax policy that uses learned patterns significantly increases playing strength. The result is MoHex 2.0, about 250 Elo points stronger than MoHex on the 11 × 11 board, and 300 Elo points stronger on the 13 × 13 board. {\textcopyright} 2014 Springer International Publishing Switzerland.},
author = {Huang, Shih Chieh and Arneson, Broderick and Hayward, Ryan B. and M{\"{u}}ller, Martin and Pawlewicz, Jakub},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-09165-5_6},
isbn = {9783319091648},
issn = {16113349},
title = {{MoHex 2.0: A pattern-based MCTS Hex player}},
year = {2014}
}
@misc{John2018,
author = {John, Anto},
title = {{Compare two deep learning frameworks: TensorFlow and Pytorch}},
url = {https://developer.ibm.com/blogs/two-famous-deep-learning-frameworks-compared-tensorflow-vs-pytorch/},
urldate = {2019-05-11},
year = {2018}
}
@misc{Kennedy2017,
author = {Kennedy, Patrick},
title = {{Case Study on the Google TPU and GDDR5 from Hot Chips 29}},
url = {https://www.servethehome.com/case-study-google-tpu-gddr5-hot-chips-29/},
urldate = {2019-05-07},
year = {2017}
}
@incollection{Kocsis2006,
abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
author = {Kocsis, Levente and Szepesv{\'{a}}ri, Csaba},
doi = {10.1007/11871842_29},
title = {{Bandit Based Monte-Carlo Planning}},
year = {2006}
}
@misc{McCarthy2006,
author = {McCarthy, John},
title = {{The Dartmouth Workshop--as planned and as it happened}},
url = {http://www-formal.stanford.edu/jmc/slides/dartmouth/dartmouth/node1.html},
urldate = {2019-05-05},
year = {2006}
}
@inproceedings{Micikevicius2018,
abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.},
address = {Vancouver, BC, Canada},
archivePrefix = {arXiv},
arxivId = {1710.03740},
author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
booktitle = {Proceedings of the 6th International Conference on Learning Representations (ICLR 2018)},
eprint = {1710.03740},
keywords = {Convolutional neural networks,Half precision,Recurrent neural networks,float16},
mendeley-tags = {Convolutional neural networks,Half precision,Recurrent neural networks,float16},
title = {{Mixed Precision Training}},
url = {http://arxiv.org/abs/1710.03740},
year = {2018}
}
@misc{NVIDIACorporation2018,
abstract = {Nvidia Turing Architecture},
author = {{NVIDIA Corporation}},
booktitle = {White Paper},
institution = {Nvidia},
title = {{NVIDIA Turing GPU Architecture}},
url = {https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf},
urldate = {2019-05-08},
year = {2018}
}
@inproceedings{Orhan2018,
abstract = {Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the "ghosts" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.},
address = {Vancouver, BC, Canada},
archivePrefix = {arXiv},
arxivId = {1701.09175},
author = {Orhan, A. Emin and Pitkow, Xaq},
booktitle = {Proceedings of the 6th International Conference on Learning Representations (ICLR 2018)},
eprint = {1701.09175},
keywords = {deep learning,optimization,skip connections},
mendeley-tags = {deep learning,optimization,skip connections},
title = {{Skip Connections Eliminate Singularities}},
url = {http://arxiv.org/abs/1701.09175},
year = {2018}
}
@incollection{Russell2010,
abstract = {The long-anticipated revision of this {\#}1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications.Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics.For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.},
author = {Russell, Stuart and Norvig, Peter},
booktitle = {Artificial Intelligence - A Modern Approach (3rd ed)},
chapter = {1},
isbn = {0136042597},
issn = {00070610},
pages = {1--29},
pmid = {8639360},
publisher = {Pearson Education, Inc},
title = {{Introduction}},
year = {2010}
}
@incollection{Sedgewick2011,
author = {Sedgewick, Robert and Wayne, Kevin},
booktitle = {Algorithms, Fourth Edition},
chapter = {1},
doi = {10.1002/1521-3773(20010316)40:6<9823::AID-ANIE9823>3.3.CO;2-C},
isbn = {9780321573513},
issn = {14337851},
publisher = {Addison-Wesley},
title = {{Fundamentals}},
year = {2011}
}
@article{Shannon1953,
abstract = {This paper reviews briefly some of the recent developments in the field of automata and nonnumerical computation. A number of typical machines are described, including logic machines, game-playing machines and learning machines. Some theoretical questions and developments are discussed, such as a comparison of computers and the brain, Turing's formulation of computing machines and von Neumann's models of self-reproducing machines.},
author = {Shannon, Claude E.},
doi = {10.1109/JRPROC.1953.274273},
issn = {00968390},
journal = {Proceedings of the IRE},
title = {{Computers and Automata}},
year = {1953}
}
@article{Silver2018,
abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1126/science.aar6404},
issn = {0036-8075},
journal = {Science},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
year = {2018}
}
@article{Silver2017,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
issn = {14764687},
journal = {Nature},
title = {{Mastering the game of Go without human knowledge}},
year = {2017}
}
@incollection{Mitchell1997,
author = {{Tom M. Mitchell}},
booktitle = {Machine Learning},
chapter = {1},
edition = {1st},
pages = {1--19},
publisher = {McGraw-Hill Science},
title = {{Introduction}},
year = {1997}
}
@inproceedings{Xie2017,
abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
author = {Xie, Saining and Girshick, Ross and Doll{\'{a}}r, Piotr and Tu, Zhuowen and He, Kaiming},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.634},
isbn = {9781538604571},
pages = {5987--5995},
title = {{Aggregated residual transformations for deep neural networks}},
volume = {2017-Janua},
year = {2017}
}
